# Langfuse Duplicate Trace Fix

## Problem

You were seeing **TWO separate traces** for each conversation:

1. **"ChatOpenAI" trace** - Created by CallbackHandler
   - Only shows 1 LLM call (synthesizer)
   - Orphaned/separate from main trace
   - Has token counts

2. **"chat.turn" trace** - Your manually created trace
   - Shows full hierarchy
   - Properly organized observations
   - Missing token counts on LLM calls

### Visual Representation

**Before (Duplicate Traces):**
```
Trace 1: "ChatOpenAI" (from CallbackHandler)
â””â”€ synthesizer.llm [GENERATION] â† Duplicate!
   â””â”€ Token counts âœ…

Trace 2: "chat.turn" (manually created)
â”œâ”€ chat.turn [SPAN]
â”‚  â”œâ”€ planner.llm [GENERATION] â† No token counts âŒ
â”‚  â”œâ”€ tool.execute.fetch_tasks [SPAN]
â”‚  â”‚  â”œâ”€ tool.request [EVENT]
â”‚  â”‚  â””â”€ tool.response [EVENT]
â”‚  â””â”€ synthesizer.llm [GENERATION] â† No token counts âŒ
```

## Root Cause

**Mixing two tracing approaches:**

1. **Manual observation creation** (lines 333-338, 540-545):
   ```python
   planner_generation = root_span.start_observation(
       name="planner.llm",
       as_type="generation",
       ...
   )
   ```

2. **CallbackHandler** (lines 351-362, 556-567):
   ```python
   langfuse_handler = langfuse_config.get_callback_handler(...)
   response = orchestrator.llm.invoke(
       messages,
       config={"callbacks": [langfuse_handler]}  # Creates separate trace!
   )
   ```

When you use **both**, you get:
- âŒ Duplicate observations (one manual, one from callback)
- âŒ Separate "ChatOpenAI" trace
- âŒ Confusing trace hierarchy
- âŒ Split metrics across two traces

## The Fix

**Removed CallbackHandler usage** because we're already manually creating observations.

### Planner LLM (Lines 343-348)

**Before:**
```python
# Get Langfuse callback handler for planner
langfuse_handler = None
if client and root_span:
    try:
        langfuse_handler = langfuse_config.get_callback_handler(
            trace_id=trace_id,
            user_id=session_id
        )
    except Exception as e:
        logger.warning(f"Failed to get Langfuse callback handler: {e}")

# Call LLM with Langfuse tracing
if langfuse_handler:
    response = orchestrator.llm.invoke(
        messages,
        config={"callbacks": [langfuse_handler]}  # âŒ Creates duplicate
    )
else:
    response = orchestrator.llm.invoke(messages)
```

**After:**
```python
# Call LLM for planning
logger.info("Calling LLM for planning")

# Note: We don't use CallbackHandler here because we're manually creating
# the observation above (planner_generation). Using both would create duplicates.
response = orchestrator.llm.invoke(messages)  # âœ… Clean, no duplicates
```

### Synthesizer LLM (Lines 533-536)

**Before:**
```python
# Get Langfuse callback handler for synthesizer
langfuse_handler = None
if client and root_span:
    try:
        langfuse_handler = langfuse_config.get_callback_handler(...)
    except Exception as e:
        logger.warning(f"Failed to get Langfuse callback handler: {e}")

# Call LLM with Langfuse tracing
if langfuse_handler:
    response = orchestrator.llm.invoke(
        messages,
        config={"callbacks": [langfuse_handler]}  # âŒ Creates duplicate
    )
else:
    response = orchestrator.llm.invoke(messages)
```

**After:**
```python
# Call LLM for final response
# Note: We don't use CallbackHandler here because we're manually creating
# the observation above (synthesizer_generation). Using both would create duplicates.
response = orchestrator.llm.invoke(messages)  # âœ… Clean, no duplicates
```

## Expected Result

**After (Single Clean Trace):**
```
Trace: "chat.turn"
â””â”€ chat.turn [SPAN]
   â”œâ”€ planner.llm [GENERATION]
   â”‚  â”œâ”€ Input: Messages
   â”‚  â”œâ”€ Output: Decision JSON
   â”‚  â””â”€ Prompt: Linked âœ…
   â”‚
   â”œâ”€ tool.execute.fetch_tasks [SPAN]
   â”‚  â”œâ”€ tool.request [EVENT]
   â”‚  â””â”€ tool.response [EVENT]
   â”‚
   â””â”€ synthesizer.llm [GENERATION]
      â”œâ”€ Input: Messages + Results
      â”œâ”€ Output: Final response
      â””â”€ Prompt: Linked âœ…
```

## Why This Works

### Manual Observation Creation
When you do:
```python
generation = root_span.start_observation(
    name="planner.llm",
    as_type="generation",
    input={...}
)
# ... LLM call ...
generation.update(output=response)
generation.end()
```

You get:
- âœ… Full control over the observation
- âœ… Proper nesting in your trace
- âœ… Custom metadata and naming
- âš ï¸ **BUT**: No automatic token counting from LangChain

### CallbackHandler
When you use:
```python
response = llm.invoke(messages, config={"callbacks": [handler]})
```

You get:
- âœ… Automatic token counting
- âœ… Automatic input/output capture
- âŒ **BUT**: Creates its own trace ("ChatOpenAI")
- âŒ **AND**: Not nested in your custom trace

### The Trade-off

We chose **manual observation creation** because:
1. âœ… Clean, single trace hierarchy
2. âœ… Full control over trace structure
3. âœ… Proper nesting and organization
4. âš ï¸ Missing automatic token counts (but Ollama doesn't report them anyway)

## Note on Token Counts

**Important**: Ollama models (gemma3:4b) **don't report token counts** through the OpenAI-compatible API, so you wouldn't get token counts even with the CallbackHandler.

If you switch to a real OpenAI model later, you could:

**Option A**: Keep manual observations (cleaner)
```python
# Manually add token counts after LLM call
if hasattr(response, 'usage'):
    generation.update(
        usage={
            "input": response.usage.prompt_tokens,
            "output": response.usage.completion_tokens
        }
    )
```

**Option B**: Use only CallbackHandler (simpler but less control)
```python
# Remove manual start_observation()
# Just use callback handler
response = llm.invoke(messages, config={"callbacks": [handler]})
# No manual update/end needed
```

## When to Use Each Approach

### Use Manual Observations When:
- âœ… You need custom trace hierarchy
- âœ… You want specific observation names
- âœ… You need to add custom metadata
- âœ… You're using Ollama (no token counts anyway)
- âœ… You want full control

### Use CallbackHandler When:
- âœ… You want automatic tracing with zero code
- âœ… You're using OpenAI/Anthropic (token counts work)
- âœ… You don't need custom trace structure
- âœ… Simple is more important than control

### NEVER Use Both Together:
- âŒ Creates duplicate traces
- âŒ Confusing hierarchy
- âŒ Split metrics
- âŒ Wasted Langfuse quota

## Testing

After the fix:
1. âœ… Send a message through your agent
2. âœ… Check Langfuse dashboard
3. âœ… Verify you see **only ONE trace** named "chat.turn"
4. âœ… Verify all observations are nested properly
5. âœ… Verify **NO "ChatOpenAI" trace** appears

Example test:
```bash
curl -X POST http://localhost:5001/api/agent/conversation \
  -H "Content-Type: application/json" \
  -d '{
    "input_text": "Fetch my tasks",
    "session_id": "test",
    "domain": "productivity"
  }'
```

Then check Langfuse - you should see **exactly one** clean trace! ğŸ‰

## Summary

**What we fixed:**
- âœ… Removed duplicate CallbackHandler usage
- âœ… Kept manual observation creation for full control
- âœ… Single clean trace in Langfuse
- âœ… Proper hierarchy with all observations nested

**Result:**
- One "chat.turn" trace
- Clean hierarchy
- No duplicate "ChatOpenAI" traces
- All observations properly organized

This gives you the clean, organized trace structure you expected!
